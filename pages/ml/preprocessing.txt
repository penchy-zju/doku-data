======= Scikit-learn之数据预处理及特征选择 =======
''Scikit-learn''作为一款开源免费的机器学习''python''库，受到了非常广泛的欢迎，
这个原作为''2007''年''Google Summer of Code''上的项目经过近十年的发展，也在''2017-06-20''
迎来了自己最新的一个稳定版：''0.18.2''，但本文中所有的''scikit-learn''代码基于''0.18.1''，
一般而言，本文中的所有代码也都可以在最新的版本中正确运行。
\\
\\
本文主要介绍如何使用''Scikit-learn''在机器学习的数据准备阶段进行处理，包括但不限于特征抽取，
数据预处理以及特征选择；每一段代码都试图保证可以运行，文中的环境如下：
  * ''python - 2.7.13''
  * ''numpy - 1.11.3''
  * ''scikit-learn - 0.18.1''

===== 特征提取 =====
''sklearn.feature_extraction''包提供用于从文本或者图像中抽取特征的工具，抽取的特征以方便机器学习算法应用的形式存在。
==== 从字典载入数据 ====
使用''DictVectorizer''可以轻松从''python''字典载入数据，并将之转化为适合于''Scikit-learn''中的''estimators''对象，如下例所示：
<code python>
measurements = [
    {'city': 'Dubai', 'temperature': 33.},
    {'city': 'London', 'temperature': 12.},
    {'city': 'San Fransisco', 'temperature': 18.},
]

from sklearn.feature_extraction import DictVectorizer
vec = DictVectorizer()

vec.fit_transform(measurements).toarray()
# array([[  1.,   0.,   0.,  33.],
#       [  0.,   1.,   0.,  12.],
#       [  0.,   0.,   1.,  18.]])

vec.get_feature_names()
# ['city=Dubai', 'city=London', 'city=San Fransisco', 'temperature']
</code>
如上代码所示，对于字典中的类别属性，''DictVectorizer''会对该属性进行''one-hot''编码，即对于''N''个值得类别属性会编码为''N''位的二进制码，对应位如果是''1''表示是该类别属性，反之则为''0''。

==== 文本特征抽取 ====
=== BoW ===
词袋模型''Bag of Words''在文本分析中的应用十分普遍，使用''scikit-learn''可以方便的从文本中获取到词袋模型特征，这个过程包括以下三个步骤：
  * ''Tokenizing''，对于英文文本来说，一般单词用空格隔开，可以方便的获取到''token''，对于中文来说，需要借助一定的分词工具才能获取到''token''；
  * ''Counting''，获取到文本''token''以后需要进一步获取各个''token''出现的次数；
  * ''Normalizing''，归一化，需要对不同''token''的权重进行归一化；
通过词袋模型，就可以把一段文本变化为一个向量，进一步应用于聚类、分类等场景。如下例所示：
<code python>
from sklearn.feature_extraction.text import CountVectorizer

vectorizer = CountVectorizer(min_df=1)
corpus = [
    'This is the first document.',
    'This is the second second document.',
    'And the third one.',
    'Is this the first document?',
]
X = vectorizer.fit_transform(corpus)
vectorizer.get_feature_names() == (
     ['and', 'document', 'first', 'is', 'one',
     'second', 'the', 'third', 'this'])
# True

X.toarray()           
# array([[0, 1, 1, 1, 0, 0, 1, 0, 1],
#        [0, 1, 0, 1, 0, 2, 1, 0, 1],
#        [1, 0, 0, 0, 1, 0, 1, 1, 0],
#        [0, 1, 1, 1, 0, 0, 1, 0, 1]]...)
</code>
=== TF-IDF ===
''TF-IDF''是建立在词袋模型之上的文本特征处理方法，其计算包括以下几个步骤：
  * 逆文档频率计算：''idf(t)=log{(1+n<sub>d</sub>)/(1+df(d,t))}+1''，其中''n<sub>d</sub>''表示总的文档数量，''df(d,t)''表示包含单词''t''的文档的数量；由该公式可以得知，如果一个词在所有文档中都出现过，那么它的逆文档频率值最小；
  * ''TF-IDF''的计算通过词频与逆文档频率相乘获得；
  * 在获取了一个文档的''TF-IDF''特征后，需要进一步的进行归一化，一般使用''l<sub>2</sub>''进行归一化；
在''scikit-learn''中，''TfidfTransformer''和''TfidfVectorizer''都可以用来获取文档的''TF-IDF''特征，不同的是，前者输入的是词袋模型向量化后的数据，而后者直接可以通过文本中获取特征，如下例所示：
<code python>
from sklearn.feature_extraction.text import TfidfTransformer
transformer = TfidfTransformer(smooth_idf=False)

counts = [[3, 0, 1],
          [2, 0, 0],
          [3, 0, 0],
          [4, 0, 0],
          [3, 2, 0],
          [3, 0, 2]]

tfidf = transformer.fit_transform(counts)
tfidf.toarray() 
# array([[ 0.81940995,  0.        ,  0.57320793],
#        [ 1.        ,  0.        ,  0.        ],
#        [ 1.        ,  0.        ,  0.        ],
#        [ 1.        ,  0.        ,  0.        ],
#        [ 0.47330339,  0.88089948,  0.        ],
#        [ 0.58149261,  0.        ,  0.81355169]])
</code>

<alert type="info" icon="glyphicon glyphicon-hand-right">
词袋模型的缺点在于忽略了文本的先后顺序，且对错误拼写敏感；前文中描述的词袋模型其实是基于''1-grams''，而通过调整''CountVectorizer''对象的''ngram_range''参数可以达到''N-grams''的效果，以缓解词袋模型的不足。
</alert>

===== 数据预处理 =====
''sklearn.preprocessing''包提供了机器学习数据预处理的绝大部分工具，使用这些工具可以轻松对数据进行处理，以便使机器学习模型达到最好的效果。
==== 标准化 ====
标准化意在使数据服从标准的正态分布，即均值为''0''，方差为''1''，如下：
<code python>
from sklearn import preprocessing
import numpy as np
X = np.array([[ 1., -1.,  2.],
              [ 2.,  0.,  0.],
              [ 0.,  1., -1.]])
scaler = preprocessing.StandardScaler().fit(X)
scaler.transform(X)
# array([[ 0.  ..., -1.22...,  1.33...],
#        [ 1.22...,  0.  ..., -0.26...],
#        [-1.22...,  1.22..., -1.06...]])
</code>
其对每一维度的数据依据公式''x=(x-x<sub>mean</sub>)/v''进行变换；
\\  \\ 
另外一种常用的数据预处理方式是将某一维度的数据全部变换到某一个范围里，''scikit-learn''中可以方便的使用''MinMaxScaler''和''MaxAbsScaler''实现，前者变换到''[0,1]''，后者变换到''[-1,1]''。
==== 归一化 ====
归一化使单一数据点变换到单位范数，如下例：
<code python>
X = [[ 1., -1.,  2.],
     [ 2.,  0.,  0.],
     [ 0.,  1., -1.]]

normalizer = preprocessing.Normalizer().fit(X)
normalizer.transform(X)  
# array([[ 0.40..., -0.40...,  0.81...],
#        [ 1.  ...,  0.  ...,  0.  ...],
#        [ 0.  ...,  0.70..., -0.70...]])
</code>

<alert type="info" icon="glyphicon glyphicon-hand-right">标准化针对数据的某一维度进行操作，而归一化则针对单一数据点的所有维度进行操作。</alert>
==== 二值化 ====
''scikit-learn''中使用''Binarizer''实现数值二值化，可以自定义指定阈值：
<code python>
X = [[ 1., -1.,  2.],
     [ 2.,  0.,  0.],
     [ 0.,  1., -1.]]
     
binarizer = preprocessing.Binarizer(threshold=1.1)
binarizer.transform(X)
# array([[ 0.,  0.,  1.],
#        [ 1.,  0.,  0.],
#        [ 0.,  0.,  0.]])
</code>
==== 缺省值处理 ====
缺省值处理使用''Imputer''，可以指定缺省值为当前行或者列的最大值、最小值、均值、中值等：
<code python>
import numpy as np
from sklearn.preprocessing import Imputer
imp = Imputer(missing_values='NaN', strategy='mean', axis=0)
imp.fit([[1, 2], [np.nan, 3], [7, 6]])
# Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)
X = [[np.nan, 2], [6, np.nan], [7, 6]]
print(imp.transform(X)) 
# [[ 4.          2.        ]
#  [ 6.          3.666...]
#  [ 7.          6.        ]]
</code>
==== 类别属性编码 ====
类别属性可以通过''OneHotEncoder''进行二进制编码：
<code python>
enc = preprocessing.OneHotEncoder()
enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  
# OneHotEncoder(categorical_features='all', dtype=<... 'numpy.float64'>,
#        handle_unknown='error', n_values='auto', sparse=True)
enc.transform([[0, 1, 3]]).toarray()
# array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])
</code>
==== 多项式特征处理 ====
有时候为了得到一些更加复杂的特征，可以通过多项式生成的方法获取新的特征，以获取特征之间的非线性性，''PolynomialFeatures''可以方便实现：
<code python>
import numpy as np
from sklearn.preprocessing import PolynomialFeatures
X = np.arange(6).reshape(3, 2)
X                                                 
# array([[0, 1],
#        [2, 3],
#        [4, 5]])
poly = PolynomialFeatures(2)
poly.fit_transform(X)                             
# array([[  1.,   0.,   1.,   0.,   0.,   1.],
#        [  1.,   2.,   3.,   4.,   6.,   9.],
#        [  1.,   4.,   5.,  16.,  20.,  25.]])
</code>
以上将特征''(x<sub>1</sub>, x<sub>2</sub>)''，变换为''(1, x<sub>1</sub>, x<sub>2</sub>, x<sub>1</sub><sup>2</sup>, x<sub>1</sub>x<sub>2</sub>, x<sub>2</sub><sup>2</sup>)''，如果只需要特征间的交叉获取的新特征，则只需要对''PolynomialFeatures''指定参数''interaction_only=True''，如果不需要常数项，则指定参数''include_bias=False''。


<alert type="warning" icon="glyphicon glyphicon-hand-right">为什么特征多项式处理后会起作用，它的原理是什么？</alert>
===== 特征选择 =====
''sklearn.feature_selection''包可应用于特征选择，或为提高学习精度，或为提高高维数据下的学习效率。

==== 过滤低方差特征 ====
通过分别计算每一个数据维度上的方差，去除一些方差小于阈值的维度，完成特征选择。''VarianceThresh''可以完成这一工作，默认情况下，所有方差为''0''的特征都会被筛除，比如某一维度上全是同一数值的特征。
<code python>
from sklearn.feature_selection import VarianceThreshold
X = [[0, 0, 1], [0, 1, 0], [1, 0, 0], [0, 1, 1], [0, 1, 0], [0, 1, 1]]
sel = VarianceThreshold(threshold=(.8 * (1 - .8)))
sel.fit_transform(X)
# array([[0, 1],
#        [1, 0],
#        [0, 0],
#        [1, 1],
#        [1, 0],
#        [1, 1]])
</code>
==== 单变量特征选择 ====
单变量的统计测试也可以用来做特征选择，在''scikit-learn''中，这种特征选择的方法使用像使用了一个基学习器(''base estimator'')，实现了''trandform''接口，进而在此基础上完成特征选择。常用的有：
  * ''SelectKBest''，用来选择''K''个得分最高的特征；
  * ''SelectPercentile''，用来筛选满足用户指定得分分数百分比的特征；
前者选择固定数量的特征，而后者则是不固定数量的。
<code python>
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
iris = load_iris()
X, y = iris.data, iris.target
X.shape
# (150, 4)
X_new = SelectKBest(chi2, k=2).fit_transform(X, y)
X_new.shape
# (150, 2)
</code>
上例中使用卡方测试来选择最好的两个特征，在''scikit-learn''中，面对分类和回归需要使用不同的方法来做特征选择：
  * 回归：''f_regression'', ''mutual_info_regression''
  * 分类：''chi2'',''f_classif'', ''mutual_info_classif''
<alert type="danger" icon="glyphicon glyphicon-hand-right">
如果面对的是分类问题，那么不能选用回归函数去做特征选择，不然结果毫无意义！
</alert>
==== 递归特征选择 ====
递归特征选择的流程是这样的：首先选定一个基学习器，可以对特征进行打分；然后使用该学习器在训练集中使用全部特征进行训练，然后剔除特征得分绝对值最小的那一维特征继续训练，如此循环往复，完成特征选择。\\ 
''RFE''和''RFECV''都可以完成这项工作，不过后者比前者多了交叉验证；
<code python>
from sklearn.svm import SVC
from sklearn.datasets import load_digits
from sklearn.feature_selection import RFE

# Load the digits dataset
digits = load_digits()
X = digits.images.reshape((len(digits.images), -1))
y = digits.target

svc = SVC(kernel="linear", C=1)
rfe = RFE(estimator=svc, n_features_to_select=1, step=1)
rfe.fit_transform(X, y)
# (1797L, 1L)
</code>
==== 使用模型进行特征选择 ====
''SelectFromModel''可以用来配合任何包含''coef_''或者''feature_importances_''属性的学习器来完成特征选择。特别的，当提供的学习器使用了''L<sub>1</sub>''约束时，''SelectFromModel''可以选择一些打分为非''0''的特征，此时特征选择的得分阈值默认是''0.00001''，即''1e-5''。
<code python>
from sklearn.svm import LinearSVC
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
iris = load_iris()
X, y = iris.data, iris.target
X.shape
# (150, 4)
lsvc = LinearSVC(C=0.01, penalty="l1", dual=False).fit(X, y)
model = SelectFromModel(lsvc, prefit=True)
X_new = model.transform(X)
X_new.shape
# (150, 3)
</code>
\\ 
''scikit-learn''集成学习包里的一些树模型可以计算特征的重要程度，也可以用来完成特征选择：
<code python>
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.datasets import load_iris
from sklearn.feature_selection import SelectFromModel
iris = load_iris()
X, y = iris.data, iris.target
X.shape
# (150, 4)
clf = ExtraTreesClassifier()
clf = clf.fit(X, y)
clf.feature_importances_  
# array([ 0.04...,  0.05...,  0.4...,  0.4...])
model = SelectFromModel(clf, prefit=True)
X_new = model.transform(X)
X_new.shape
# (150, 2)
</code>
在上面代码中，筛选特征的打分阈值其实是所有特征得分的均值，也是默认值。
===== Reference =====
  * [[http://scikit-learn.org/stable/modules/feature_extraction.html | Feature extraction]]
  * [[http://scikit-learn.org/stable/modules/preprocessing.html | Preprocessing data]]
  * [[http://scikit-learn.org/stable/modules/feature_selection.html | Feature selection]]
  * [[http://playground.tensorflow.org | A Neural Network Playground]]