=======Adaboost 算法过程及细节=======
===== 算法流程 =====
{{ :ml:adaboost_algorithm.png |}}\\ 

  -上述算法流程中，''//I//(//G<sub>m</sub>//(x<sub>i</sub>)!=y<sub>i</sub>)''等于''0''或者''1''，即当前弱分类器''//G<sub>m</sub>//''在当前第''i''个数据点预测正确则取值为''1''，否则取值为''0''，由此可知，误差率''e<sub>m</sub>''是当前弱分类器在所有分错数据点上的**权值和**；
  -弱分类器权值''α<sub>m</sub>''的取值范围只从公式的角度考虑其取值范围是''(-∞,+∞)''，且其为正值时表示当前弱分类器的分类误差率''e<sub>m</sub>''小于''0.5''，反之则是大于''0.5''，简化考虑，当进行第一轮弱分类器训练时，''e<sub>m</sub>''大于''0.5''代表着该分类器分错了一半以上的数据；
  -对于弱分类器''G<sub>m</sub>''，如果它的误差率''e<sub>m</sub>''小于''0.5''，可以称之为“好”的分类器，反之称之为“坏”的分类器，进一步观察数据点权值''W<sub>m</sub>''更新公式，可以发现，对于一个好的分类器，在更新数据点权值时，分错的数据点权值会变大，而分类正确的数据点权值会变小；然而对于一个坏的分类器，则恰恰相反，分错的数据点权值在更新后反倒变小了，这样就不符合''boosting''的初衷了。所以在上述的算法流程中，为了达到分类错误点的权值变大，需要对弱分类器的误差率''e<sub>m</sub>''进行判断，当其值大于''0.5''的时候终止训练；